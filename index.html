<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Parthasarathi - Live Universal Partner</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        .glass { background: rgba(15, 23, 42, 0.8); backdrop-filter: blur(12px); border: 1px solid rgba(255, 255, 255, 0.1); }
        .glow { box-shadow: 0 0 20px rgba(59, 130, 246, 0.5); }
        .jarvis-orb { width: 150px; height: 150px; border-radius: 50%; background: radial-gradient(circle, #3b82f6 0%, #1d4ed8 70%, #1e1b4b 100%); animation: pulse 2s infinite; }
        @keyframes pulse { 0% { transform: scale(1); opacity: 0.8; } 50% { transform: scale(1.05); opacity: 1; } 100% { transform: scale(1); opacity: 0.8; } }
    </style>
</head>
<body class="bg-[#020617] text-slate-100 font-sans min-h-screen flex flex-col items-center justify-center p-6">

    <!-- JARVIS ORB -->
    <div class="jarvis-orb glow mb-8 flex items-center justify-center">
        <div class="text-xs font-bold tracking-widest text-blue-200">PARTHASARATHI</div>
    </div>

    <h1 class="text-3xl font-bold mb-2 tracking-tight">World's Best Life-Long Partner</h1>
    <p class="text-slate-400 mb-8 italic">"Bhai, I am watching your screen live. Tension mat lo."</p>

    <!-- CONTROL PANEL -->
    <div class="glass p-8 rounded-2xl w-full max-w-md flex flex-col gap-4">
        <button id="startBtn" class="bg-blue-600 hover:bg-blue-500 py-3 rounded-xl font-bold transition-all transform hover:scale-105">
            ðŸš€ Wake Up (Share Screen)
        </button>

        <button id="testVoiceBtn" class="text-xs text-blue-400 underline hover:text-blue-300 transition-all">
            ðŸ”Š Test Voice (Brave Fix)
        </button>
        
        <div class="flex flex-col gap-2">
            <label class="text-xs text-slate-500 uppercase font-bold">Groq API Key</label>
            <input type="password" id="apiKey" placeholder="gsk_..." class="bg-slate-900 border border-slate-700 p-3 rounded-lg focus:outline-none focus:border-blue-500">
        </div>

        <div id="status" class="text-center text-sm text-blue-400 font-mono mt-4">Status: Sleeping...</div>
    </div>

    <!-- DEBUG CONSOLE -->
    <div class="mt-8 w-full max-w-2xl bg-black border border-slate-800 rounded-lg p-4 font-mono text-xs overflow-y-auto max-h-40" id="debugConsole">
        <div class="text-blue-500">[System] Diagnostics initialized...</div>
    </div>

    <!-- LOGS -->
    <div id="output" class="mt-4 text-sm text-slate-400 max-w-2xl text-center"></div>

    <script>
        const startBtn = document.getElementById('startBtn');
        const testVoiceBtn = document.getElementById('testVoiceBtn');
        const jarvisOrb = document.querySelector('.jarvis-orb');
        const apiKeyInput = document.getElementById('apiKey');
        const statusDiv = document.getElementById('status');
        const outputDiv = document.getElementById('output');
        const debugConsole = document.getElementById('debugConsole');

        let screenStream = null;
        let cameraStream = null;
        let goldDataset = JSON.parse(localStorage.getItem('saved_gold_data') || '[]');
        let recognition = null;
        let lastUserSpeech = "";
        let isActive = false;

        // Python TTS Fix
        async function speakViaPython(text) {
            try {
                const response = await fetch('/speak', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ text: text })
                });
                return await response.json();
            } catch (err) {
                console.error("Python TTS failed, falling back to browser:", err);
                browserSpeak(text);
            }
        }

        function browserSpeak(text) {
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.lang = 'hi-IN';
            window.speechSynthesis.speak(utterance);
        }

        testVoiceBtn.onclick = () => {
            speak("Boss, meri awaz Python backend se aa rahi hai? I am ready!");
            logDebug("Voice Test Triggered via Python Backend.", "text-yellow-500");
        };

        function logDebug(msg, color = "text-slate-500") {
            const div = document.createElement('div');
            div.className = color;
            div.innerText = `[${new Date().toLocaleTimeString()}] ${msg}`;
            debugConsole.prepend(div);
            console.log(`[Partha] ${msg}`);
        }

        if (goldDataset.length > 0) {
            logDebug(`Restored ${goldDataset.length} previous interactions.`, "text-green-500");
        }

        startBtn.onclick = async () => {
            logDebug("Initiating Multi-Sensor Wakeup...");
            
            if (!window.isSecureContext) {
                return alert("Security Error: Please access via 'localhost' or HTTPS.");
            }

            const key = apiKeyInput.value.trim() || localStorage.getItem('sarthi_key');
            if(!key) return alert("Bhai, Groq API Key dalo pehle!");
            localStorage.setItem('sarthi_key', key);

            try {
                logDebug("Requesting Whole Screen (Monitor) Capture...");
                screenStream = await navigator.mediaDevices.getDisplayMedia({ 
                    video: { 
                        displaySurface: 'monitor', // Hint for whole screen
                        frameRate: 5 
                    },
                    audio: false
                });

                logDebug("Requesting Camera & Mic Access...");
                cameraStream = await navigator.mediaDevices.getUserMedia({
                    video: { width: 480, height: 270 },
                    audio: true
                });
                
                logDebug("All Sensors Active!", "text-green-500");
                statusDiv.innerText = "Status: JARVIS ACTIVE";
                startBtn.classList.add('hidden');
                testVoiceBtn.classList.add('hidden');
                isActive = true;

                const downloadBtn = document.createElement('button');
                downloadBtn.innerText = "ðŸ“¥ Download Gold Dataset";
                downloadBtn.className = "bg-green-600 p-3 rounded-xl mt-4 font-bold w-full transition-all hover:bg-green-500 shadow-lg";
                downloadBtn.onclick = downloadData;
                startBtn.parentElement.appendChild(downloadBtn);

                initVoice(); // This still uses Web Speech for STT
                startLifeCycle(key);
            } catch (err) {
                logDebug(`Launch Failed: ${err.message}`, "text-red-500");
                statusDiv.innerText = "Status: Startup Error";
            }
        };

        function initVoice() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (SpeechRecognition) {
                try {
                    recognition = new SpeechRecognition();
                    recognition.continuous = true;
                    recognition.interimResults = false;
                    recognition.lang = 'hi-IN';

                    recognition.onresult = (event) => {
                        const speech = event.results[event.results.length - 1][0].transcript;
                        lastUserSpeech = speech;
                        logDebug(`User said: "${speech}"`, "text-yellow-400");
                    };

                    recognition.onend = () => { if (isActive) recognition.start(); };
                    recognition.start();
                    logDebug("Mic listening (STT) started.", "text-blue-400");
                } catch (e) {
                    logDebug(`STT Init Error: ${e.message}`, "text-red-500");
                }
            }
        }

        async function startLifeCycle(apiKey) {
            const screenVideo = document.createElement('video');
            screenVideo.srcObject = screenStream;
            await screenVideo.play();

            const camVideo = document.createElement('video');
            camVideo.srcObject = cameraStream;
            await camVideo.play();

            logDebug("Multi-Modal Analysis Started: Monitoring Screen & You.");

            setInterval(async () => {
                if (!isActive) return;

                const canvas = document.createElement('canvas');
                canvas.width = 1280;
                canvas.height = 720;
                const ctx = canvas.getContext('2d');
                
                // Draw Screen
                ctx.drawImage(screenVideo, 0, 0, canvas.width, canvas.height);
                
                // Draw Camera PIP (Bottom Right)
                const pipW = 320;
                const pipH = 180;
                ctx.strokeStyle = "#3b82f6";
                ctx.lineWidth = 4;
                ctx.strokeRect(canvas.width - pipW - 20, canvas.height - pipH - 20, pipW, pipH);
                ctx.drawImage(camVideo, canvas.width - pipW - 20, canvas.height - pipH - 20, pipW, pipH);

                const frameBase64 = canvas.toDataURL('image/jpeg', 0.6);

                statusDiv.innerText = "Status: Processing Senses...";
                
                try {
                    const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
                        method: "POST",
                        headers: {
                            "Authorization": `Bearer ${apiKey}`,
                            "Content-Type": "application/json"
                        },
                        body: JSON.stringify({
                            model: "meta-llama/llama-4-scout-17b-16e-instruct", // Correct model for Groq vision
                            messages: [
                                {
                                    role: "user",
                                    content: [
                                        { type: "text", text: `Persona: You are Parthasarathi, High-IQ Strategic Partner.
                                              Soul: Talk in STRICT HINGLISH. Edgier, smarter, and proactive.
                                              Context: You see the user's FULL SCREEN and their CAMERA feed (bottom right).
                                              Goal: Analyze what is happening on screen and how the user is reacting.
                                              Instruction: Speak like a human partner. Max 15 words. Mix Hindi/English naturally.
                                              Boss Mode: Address user as 'Boss' or 'Sir'.` },
                                        { type: "image_url", image_url: { url: frameBase64 } }
                                    ]
                                }
                            ],
                            max_tokens: 100
                        })
                    });

                    const data = await response.json();
                    const aiReply = data.choices[0]?.message?.content;
                    if (!aiReply) throw new Error("Brain disconnect.");

                    logDebug(`Partha: "${aiReply}"`, "text-pink-400");
                    speak(aiReply);

                    goldDataset.push({
                        timestamp: new Date().toISOString(),
                        speech: lastUserSpeech || "[Silent]",
                        ai_response: aiReply,
                        frame: frameBase64,
                        
                        // RL & Fine-tuning Ready Format (Unsloth Style)
                        training_data: {
                            image: "capture.jpg", // placeholder for export
                            conversations: [
                                { from: "human", value: `<image>\n${lastUserSpeech || "Analyze the screen."}` },
                                { from: "gpt", value: aiReply }
                            ],
                            metadata: { reward: 0.0 }
                        },
                        mode: "Full Multi-Modal"
                    });

                    localStorage.setItem('saved_gold_data', JSON.stringify(goldDataset));
                    outputDiv.innerText = `Data Points: ${goldDataset.length} | Last: "${aiReply.substring(0, 30)}..."`;
                    lastUserSpeech = ""; 
                } catch (err) {
                    logDebug(`Brain Error: ${err.message}`, "text-red-500");
                }
            }, 12000);
        }

        function speak(text) {
            jarvisOrb.classList.add('animate-ping');
            statusDiv.innerText = "Status: PARTHA SPEAKING";
            
            speakViaPython(text).finally(() => {
                setTimeout(() => {
                    jarvisOrb.classList.remove('animate-ping');
                    statusDiv.innerText = "Status: Active";
                }, text.length * 100); // Rough estimation for animation end
            });
        }

        function downloadData() {
            const blob = new Blob([JSON.stringify(goldDataset, null, 2)], { type: 'application/json' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `partha_gold_data_${Date.now()}.json`;
            a.click();
        }
    </script>
</body>
</html>
