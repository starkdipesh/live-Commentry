# ğŸ¯ Custom AI Model Training Roadmap
# Path to Building Your Personal Gameplay Vision Model

## ğŸ—ºï¸ Training Pathway Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   YOUR TRAINING JOURNEY                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Phase 1: Data Collection (2-4 weeks)                           â”‚
â”‚  â”œâ”€ Collect 1000+ gameplay screenshots                          â”‚
â”‚  â”œâ”€ Write quality commentary for each                           â”‚
â”‚  â””â”€ Organize and annotate dataset                               â”‚
â”‚                                                                  â”‚
â”‚  Phase 2: Infrastructure Setup (1 week)                         â”‚
â”‚  â”œâ”€ Setup GPU environment (cloud or local)                      â”‚
â”‚  â”œâ”€ Install training frameworks                                 â”‚
â”‚  â””â”€ Prepare training pipeline                                   â”‚
â”‚                                                                  â”‚
â”‚  Phase 3: Model Selection & Baseline (3-5 days)                 â”‚
â”‚  â”œâ”€ Choose base model (LLaVA/Qwen-VL/BLIP-2)                    â”‚
â”‚  â”œâ”€ Test baseline performance                                   â”‚
â”‚  â””â”€ Establish evaluation metrics                                â”‚
â”‚                                                                  â”‚
â”‚  Phase 4: Fine-Tuning (1-2 weeks)                               â”‚
â”‚  â”œâ”€ LoRA fine-tuning (Parameter-Efficient)                      â”‚
â”‚  â”œâ”€ Full fine-tuning (if resources allow)                       â”‚
â”‚  â””â”€ Iterative improvement                                       â”‚
â”‚                                                                  â”‚
â”‚  Phase 5: Evaluation & Optimization (1 week)                    â”‚
â”‚  â”œâ”€ Benchmark against baseline                                  â”‚
â”‚  â”œâ”€ Optimize inference speed                                    â”‚
â”‚  â””â”€ Quantization for deployment                                 â”‚
â”‚                                                                  â”‚
â”‚  Phase 6: Deployment (3-5 days)                                 â”‚
â”‚  â”œâ”€ Convert to Ollama format                                    â”‚
â”‚  â”œâ”€ Integrate into commentary system                            â”‚
â”‚  â””â”€ Production testing                                          â”‚
â”‚                                                                  â”‚
â”‚  TOTAL TIME: 6-10 weeks                                         â”‚
â”‚  COST: $200-$800 (using cloud GPU)                              â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š PHASE 1: Data Collection (Most Important!)

### 1.1 Dataset Requirements

**Minimum Dataset:**
- 500 samples (bare minimum for LoRA)
- 1000+ samples (recommended for good results)
- 5000+ samples (professional quality)

**Sample Structure:**
```json
{
  "image": "screenshot_001.jpg",
  "commentary": "à¤¯à¥‹! Perfect headshot - enemy down!",
  "game": "Valorant",
  "scene_type": "intense_action",
  "ui_elements": ["health_bar", "ammo_count", "kill_feed"],
  "actions": ["shooting", "aiming"],
  "metadata": {
    "brightness": "normal",
    "motion_level": 0.8,
    "player_state": "combat"
  }
}
```

### 1.2 Data Collection Tools

#### Automated Collector

```python
# dataset_builder.py
"""
Automated dataset collection tool for gameplay training
"""

import json
import time
from pathlib import Path
from PIL import Image
import mss

class GameplayDatasetBuilder:
    def __init__(self, output_dir="training_dataset"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.images_dir = self.output_dir / "images"
        self.images_dir.mkdir(exist_ok=True)
        
        self.samples = []
        self.sample_id = 0
    
    def collect_sample(self, 
                      screenshot: Image.Image = None,
                      commentary: str = None,
                      game_name: str = "Unknown",
                      scene_type: str = "gameplay",
                      auto_capture: bool = False):
        """
        Collect a single training sample
        
        Args:
            screenshot: Pre-captured screenshot (or None for auto-capture)
            commentary: GOOD quality commentary for this screenshot
            game_name: Game being played
            scene_type: 'menu', 'gameplay', 'action', 'cutscene', etc.
            auto_capture: If True, captures screen automatically
        """
        # Auto-capture if needed
        if screenshot is None and auto_capture:
            screenshot = self._capture_screen()
        
        if screenshot is None:
            print("âŒ No screenshot provided")
            return
        
        if not commentary:
            # Interactive mode - ask user for commentary
            commentary = input("Enter commentary for this screenshot: ")
        
        # Save image
        img_filename = f"sample_{self.sample_id:06d}.jpg"
        img_path = self.images_dir / img_filename
        screenshot.save(img_path, quality=95)
        
        # Create annotation
        sample = {
            "id": self.sample_id,
            "image": str(img_path.relative_to(self.output_dir)),
            "commentary": commentary,
            "game": game_name,
            "scene_type": scene_type,
            "timestamp": time.time(),
        }
        
        self.samples.append(sample)
        self.sample_id += 1
        
        print(f"âœ… Collected sample {self.sample_id}: {commentary[:50]}...")
        
        return sample
    
    def _capture_screen(self) -> Image.Image:
        """Capture current screen"""
        with mss.mss() as sct:
            monitor = sct.monitors[1]
            screenshot = sct.grab(monitor)
            img = Image.frombytes('RGB', screenshot.size, screenshot.bgra, 'raw', 'BGRX')
            return img
    
    def auto_collect_session(self, 
                            game_name: str,
                            duration_minutes: int = 30,
                            interval_seconds: int = 10):
        """
        Automatically collect screenshots during gameplay
        You'll add commentary later
        
        Args:
            game_name: Name of the game
            duration_minutes: How long to collect
            interval_seconds: Interval between captures
        """
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  AUTO COLLECTION MODE                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ® Game: {game_name}
â±ï¸  Duration: {duration_minutes} minutes
ğŸ“¸ Interval: {interval_seconds} seconds

Starting in 5 seconds... Get ready to play!
        """)
        
        time.sleep(5)
        
        end_time = time.time() + (duration_minutes * 60)
        
        while time.time() < end_time:
            # Capture screenshot
            screenshot = self._capture_screen()
            
            # Save with placeholder commentary
            self.collect_sample(
                screenshot=screenshot,
                commentary="[TO BE ANNOTATED]",
                game_name=game_name,
                scene_type="unknown",
            )
            
            time.sleep(interval_seconds)
        
        print(f"\nâœ… Auto-collection complete! {self.sample_id} samples collected.")
        print("ğŸ“ Next: Add commentary to each sample in annotations.json")
    
    def add_commentary_batch(self):
        """
        Interactive mode to add commentary to collected samples
        """
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  COMMENTARY ANNOTATION MODE                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ Adding commentary to {len(self.samples)} samples
Press Ctrl+C to save and exit
        """)
        
        for i, sample in enumerate(self.samples):
            if sample['commentary'] != "[TO BE ANNOTATED]":
                continue  # Skip already annotated
            
            print(f"\n{'='*60}")
            print(f"Sample {i+1}/{len(self.samples)}")
            print(f"Game: {sample['game']}")
            print(f"Image: {sample['image']}")
            
            # Show image (optional - if you have display)
            try:
                img = Image.open(self.output_dir / sample['image'])
                img.show()
            except:
                print("(Cannot display image)")
            
            # Get commentary
            commentary = input("\nEnter GOOD commentary for this screenshot: ")
            scene_type = input("Scene type (action/menu/gameplay/cutscene): ") or "gameplay"
            
            # Update sample
            sample['commentary'] = commentary
            sample['scene_type'] = scene_type
            
            print(f"âœ… Annotated: {commentary}")
        
        print("\nâœ… All samples annotated!")
        self.save_dataset()
    
    def save_dataset(self, filename="annotations.json"):
        """Save all annotations to JSON"""
        output_file = self.output_dir / filename
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'version': '1.0',
                'total_samples': len(self.samples),
                'samples': self.samples
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… Dataset saved to: {output_file}")
        print(f"   Total samples: {len(self.samples)}")
        
        # Create train/val split
        self._create_split()
    
    def _create_split(self, train_ratio=0.9):
        """Create train/validation split"""
        import random
        
        random.shuffle(self.samples)
        split_idx = int(len(self.samples) * train_ratio)
        
        train_samples = self.samples[:split_idx]
        val_samples = self.samples[split_idx:]
        
        # Save splits
        with open(self.output_dir / "train.json", 'w', encoding='utf-8') as f:
            json.dump({'samples': train_samples}, f, indent=2, ensure_ascii=False)
        
        with open(self.output_dir / "val.json", 'w', encoding='utf-8') as f:
            json.dump({'samples': val_samples}, f, indent=2, ensure_ascii=False)
        
        print(f"   Train: {len(train_samples)} samples")
        print(f"   Val: {len(val_samples)} samples")


# Usage Example:
if __name__ == "__main__":
    builder = GameplayDatasetBuilder(output_dir="my_gameplay_dataset")
    
    # Option 1: Auto-collect screenshots while playing
    builder.auto_collect_session(
        game_name="Valorant",
        duration_minutes=30,
        interval_seconds=10
    )
    
    # Option 2: Add commentary later
    # builder.add_commentary_batch()
    
    # Option 3: Manual collection (one by one)
    # builder.collect_sample(
    #     screenshot=Image.open("screenshot.png"),
    #     commentary="à¤¯à¥‹! Perfect headshot!",
    #     game_name="CS:GO",
    #     scene_type="action"
    # )
    
    # Save dataset
    builder.save_dataset()
```

**Usage:**
```bash
# Run auto-collection while playing
python dataset_builder.py

# Play your game for 30 minutes
# Screenshots will be saved automatically

# Then add commentary
python -c "
from dataset_builder import GameplayDatasetBuilder
builder = GameplayDatasetBuilder('my_gameplay_dataset')
builder.add_commentary_batch()
"
```

### 1.3 Data Quality Guidelines

**Good Commentary Examples:**
- âœ… "à¤¯à¥‹! à¤¤à¥€à¤¨ enemies simultaneously - triple kill! ğŸ”¥"
- âœ… "à¤¦à¥‡à¤–à¥‹ HP bar à¤²à¤¾à¤² - critical damage zone à¤®à¥‡à¤‚!"
- âœ… "Perfect timing à¤ªà¥‡ reload - pro move!"

**Bad Commentary Examples:**
- âŒ "Gameplay à¤šà¤² à¤°à¤¹à¤¾ à¤¹à¥ˆ"
- âŒ "Screen à¤¦à¤¿à¤– à¤°à¤¹à¤¾ à¤¹à¥ˆ"
- âŒ "à¤…à¤šà¥à¤›à¤¾ à¤²à¤— à¤°à¤¹à¤¾ à¤¹à¥ˆ"

**Quality Checklist:**
- [ ] Mentions SPECIFIC visual details
- [ ] Uses actual values (numbers, colors)
- [ ] Natural and entertaining
- [ ] Appropriate energy level
- [ ] Not generic
- [ ] 10-20 words optimal length

---

## ğŸ”§ PHASE 2: Infrastructure Setup

### 2.1 Hardware Options

#### Option A: Local GPU (If Available)

**Minimum:**
- NVIDIA RTX 3060 (12GB VRAM)
- 32GB System RAM
- 500GB SSD storage

**Recommended:**
- NVIDIA RTX 4090 (24GB VRAM)
- 64GB System RAM
- 1TB NVMe SSD

**Cost:** $1500-3000 (if buying new GPU)

#### Option B: Cloud GPU (Recommended)

**Providers & Pricing:**

1. **RunPod** (Cheapest)
   - RTX 4090: $0.79/hour
   - A100 40GB: $1.89/hour
   - A100 80GB: $2.49/hour
   - Website: runpod.io

2. **Vast.ai** (Flexible)
   - RTX 4090: $0.60-0.90/hour
   - A100: $1.50-2.00/hour
   - Spot instances available
   - Website: vast.ai

3. **Lambda Labs** (Reliable)
   - A100 40GB: $1.10/hour
   - A100 80GB: $1.99/hour
   - Website: lambdalabs.com

**Est. Cost for Full Training:**
- LoRA Fine-tuning (RTX 4090): 10-20 hours = $15-30
- Full Fine-tuning (A100 40GB): 30-50 hours = $60-100

### 2.2 Software Setup

```bash
# Create training environment
conda create -n vision-training python=3.10
conda activate vision-training

# Install PyTorch with CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install training frameworks
pip install transformers>=4.35.0
pip install peft>=0.6.0  # For LoRA
pip install accelerate>=0.24.0
pip install bitsandbytes>=0.41.0  # For quantization
pip install datasets>=2.15.0

# Install evaluation tools
pip install rouge-score
pip install bert-score
pip install sacrebleu

# Install vision-specific
pip install timm  # Vision models
pip install einops  # Tensor operations
```

---

## ğŸ§  PHASE 3: Model Selection & Baseline

### 3.1 Choose Base Model

**Option 1: LLaVA (Recommended for Beginners)**
- âœ… Well-documented
- âœ… Good performance
- âœ… Easy to fine-tune
- âŒ English-focused

```python
from transformers import LlavaForConditionalGeneration, AutoProcessor

model = LlavaForConditionalGeneration.from_pretrained(
    "llava-hf/llava-1.5-7b-hf",
    torch_dtype=torch.float16,
    device_map="auto"
)

processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")
```

**Option 2: Qwen-VL (Best for Hindi)**
- âœ… Multilingual (supports Hindi!)
- âœ… More customizable
- âœ… Better for non-English
- âŒ More complex to fine-tune

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    trust_remote_code=True
)
```

**Option 3: BLIP-2 (Lightweight)**
- âœ… Faster inference
- âœ… Lower memory
- âŒ Less detailed understanding

### 3.2 Establish Baseline

```python
# baseline_evaluation.py
"""
Test baseline model performance before fine-tuning
"""

from transformers import LlavaForConditionalGeneration, AutoProcessor
from PIL import Image
import torch

class BaselineEvaluator:
    def __init__(self, model_name="llava-hf/llava-1.5-7b-hf"):
        self.model = LlavaForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        self.processor = AutoProcessor.from_pretrained(model_name)
    
    def evaluate_sample(self, image_path, expected_commentary):
        """Evaluate a single sample"""
        image = Image.open(image_path)
        
        prompt = "Describe this gameplay screenshot as a humorous Hindi gaming commentator in 1-2 sentences:"
        
        inputs = self.processor(
            text=prompt,
            images=image,
            return_tensors="pt"
        ).to("cuda", torch.float16)
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7
        )
        
        generated = self.processor.decode(outputs[0], skip_special_tokens=True)
        
        print(f"Expected: {expected_commentary}")
        print(f"Generated: {generated}")
        print("-" * 60)
        
        return generated
    
    def evaluate_dataset(self, val_dataset):
        """Evaluate entire validation set"""
        scores = []
        
        for sample in val_dataset['samples']:
            generated = self.evaluate_sample(
                sample['image'],
                sample['commentary']
            )
            
            # Calculate similarity score (use BLEU, ROUGE, etc.)
            score = self._calculate_score(generated, sample['commentary'])
            scores.append(score)
        
        avg_score = sum(scores) / len(scores)
        print(f"\nğŸ“Š Baseline Average Score: {avg_score:.2f}")
        
        return avg_score
    
    def _calculate_score(self, generated, reference):
        """Calculate quality score (simplified)"""
        # Use ROUGE, BLEU, or human evaluation
        # Simplified word overlap for demo
        gen_words = set(generated.lower().split())
        ref_words = set(reference.lower().split())
        
        if len(gen_words) == 0:
            return 0.0
        
        overlap = len(gen_words & ref_words) / len(ref_words)
        return overlap * 100

# Run baseline evaluation
evaluator = BaselineEvaluator()
val_data = json.load(open('training_dataset/val.json'))
baseline_score = evaluator.evaluate_dataset(val_data)
```

---

## ğŸš€ PHASE 4: Fine-Tuning

### 4.1 LoRA Fine-Tuning (Recommended)

**Why LoRA?**
- âœ… Parameter-efficient (train only 1-2% of parameters)
- âœ… Faster training
- âœ… Less memory
- âœ… Multiple adapters for different games

```python
# finetune_lora.py
"""
Fine-tune LLaVA with LoRA for gameplay commentary
"""

from transformers import LlavaForConditionalGeneration, AutoProcessor, Trainer, TrainingArguments
from peft import get_peft_model, LoraConfig, TaskType
from datasets import load_dataset
import torch

# Load base model
model_name = "llava-hf/llava-1.5-7b-hf"
model = LlavaForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

processor = AutoProcessor.from_pretrained(model_name)

# Configure LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16,  # LoRA rank (higher = more parameters)
    lora_alpha=32,  # Scaling factor
    lora_dropout=0.1,
    target_modules=[
        "q_proj",  # Query projection
        "v_proj",  # Value projection
        # Optionally add more:
        # "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"
    ],
    bias="none",
)

# Apply LoRA to model
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Expected output: trainable params: ~4-8M (~1-2% of total)

# Load dataset
dataset = load_dataset('json', data_files={
    'train': 'training_dataset/train.json',
    'val': 'training_dataset/val.json'
})

# Training arguments
training_args = TrainingArguments(
    output_dir="./llava_gameplay_lora",
    num_train_epochs=5,
    per_device_train_batch_size=2,  # Adjust based on GPU memory
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=8,  # Effective batch size = 2*8 = 16
    learning_rate=2e-4,
    warmup_steps=100,
    logging_steps=10,
    eval_strategy="steps",
    eval_steps=50,
    save_steps=100,
    save_total_limit=3,
    fp16=True,  # Mixed precision
    report_to="tensorboard",
)

# Create trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["val"],
)

# Start training
print("ğŸš€ Starting LoRA fine-tuning...")
trainer.train()

# Save final model
model.save_pretrained("./llava_gameplay_lora_final")
processor.save_pretrained("./llava_gameplay_lora_final")

print("âœ… Training complete!")
```

**Run training:**
```bash
# On cloud GPU
python finetune_lora.py

# Monitor with tensorboard
tensorboard --logdir ./llava_gameplay_lora/runs

# Expected time:
# - 500 samples: 2-3 hours on RTX 4090
# - 1000 samples: 4-6 hours on RTX 4090
# - 5000 samples: 15-20 hours on A100
```

### 4.2 Dataset Preparation

```python
# prepare_dataset.py
"""
Prepare dataset for training
"""

from datasets import Dataset, DatasetDict
from PIL import Image
import json

def prepare_llava_dataset(annotations_file):
    """Convert annotations to LLaVA format"""
    
    with open(annotations_file) as f:
        data = json.load(f)
    
    examples = []
    
    for sample in data['samples']:
        # Create conversation format
        conversation = [
            {
                "role": "user",
                "content": f"<image>\nDescribe this gameplay screenshot as a humorous Hindi gaming commentator:"
            },
            {
                "role": "assistant",
                "content": sample['commentary']
            }
        ]
        
        examples.append({
            'image': sample['image'],
            'conversations': conversation,
            'game': sample['game'],
            'scene_type': sample['scene_type']
        })
    
    return Dataset.from_list(examples)

# Prepare datasets
train_dataset = prepare_llava_dataset('training_dataset/train.json')
val_dataset = prepare_llava_dataset('training_dataset/val.json')

dataset_dict = DatasetDict({
    'train': train_dataset,
    'validation': val_dataset
})

# Save in HuggingFace format
dataset_dict.save_to_disk('./llava_gameplay_dataset')
```

---

## ğŸ“Š PHASE 5: Evaluation & Optimization

### 5.1 Evaluate Fine-Tuned Model

```python
# evaluate_finetuned.py
"""
Evaluate fine-tuned model and compare to baseline
"""

from transformers import LlavaForConditionalGeneration, AutoProcessor
from peft import PeftModel
import json

# Load fine-tuned model
base_model = LlavaForConditionalGeneration.from_pretrained(
    "llava-hf/llava-1.5-7b-hf",
    torch_dtype=torch.float16,
    device_map="auto"
)

# Load LoRA weights
model = PeftModel.from_pretrained(
    base_model,
    "./llava_gameplay_lora_final"
)

processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")

# Test on validation set
val_data = json.load(open('training_dataset/val.json'))

print("ğŸ“Š Evaluating Fine-Tuned Model...\n")

for i, sample in enumerate(val_data['samples'][:10]):
    image = Image.open(sample['image'])
    
    inputs = processor(
        text="Describe this gameplay as a Hindi commentator:",
        images=image,
        return_tensors="pt"
    ).to("cuda", torch.float16)
    
    outputs = model.generate(**inputs, max_new_tokens=100)
    generated = processor.decode(outputs[0], skip_special_tokens=True)
    
    print(f"Sample {i+1}:")
    print(f"  Expected: {sample['commentary']}")
    print(f"  Generated: {generated}")
    print(f"  Quality: {'âœ… Good' if len(generated) > 20 else 'âŒ Too short'}\n")
```

### 5.2 Quantization for Faster Inference

```python
# quantize_model.py
"""
Quantize model to 4-bit for faster inference
"""

from transformers import BitsAndBytesConfig
import torch

# 4-bit quantization config
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# Load quantized model
model = LlavaForConditionalGeneration.from_pretrained(
    "./llava_gameplay_lora_final",
    quantization_config=quantization_config,
    device_map="auto"
)

print("âœ… Model quantized to 4-bit")
print(f"   Memory usage: ~4-5GB (vs ~14GB for full precision)")
print(f"   Inference: ~2x faster")
print(f"   Quality: ~95% of original")
```

---

## ğŸš¢ PHASE 6: Deployment

### 6.1 Convert to Ollama Format

```bash
# Create model file for Ollama
cat > Modelfile << 'EOF'
FROM ./llava_gameplay_lora_final

# System prompt
SYSTEM """
You are a professional Hindi gaming commentator creating LIVE streams.
Generate SHORT (1-2 sentences), specific, entertaining commentary.
Mention exact visual details: colors, numbers, actions.
Use gaming slang naturally.
"""

# Model parameters
PARAMETER temperature 0.75
PARAMETER top_k 40
PARAMETER top_p 0.92
PARAMETER num_ctx 4096

# Vision parameters
PARAMETER vision_enabled true
PARAMETER image_quality high
EOF

# Build Ollama model
ollama create gameplay-custom -f Modelfile

# Test
ollama run gameplay-custom "Describe this gameplay" < screenshot.jpg
```

### 6.2 Integration

```python
# Use in commentary system
class CustomModelCommentator(EnhancedGameplayCommentator):
    def __init__(self):
        super().__init__()
        self.model_name = "gameplay-custom"  # Your custom model!
```

---

## ğŸ“ˆ Success Metrics

### Baseline vs Fine-Tuned Comparison

| Metric | Baseline | After LoRA | After Full FT |
|--------|----------|-----------|---------------|
| Specificity | 40% | 75% | 85% |
| Hindi Quality | 50% | 80% | 90% |
| Gaming Slang | 30% | 70% | 80% |
| Variety | 45% | 75% | 85% |
| Inference Speed | 3s | 3.5s | 3.5s |

---

## ğŸ’° Total Cost Breakdown

### Budget Options:

**Minimal ($50-100):**
- Collect 500 samples yourself
- Use RunPod RTX 4090 ($0.79/hr)
- LoRA fine-tuning (10 hours)
- Total: ~$60

**Recommended ($200-300):**
- Collect 1000 samples
- Use RunPod A100 40GB ($1.89/hr)
- LoRA + experimentation (30 hours)
- Total: ~$250

**Professional ($500-800):**
- Hire annotators for 5000 samples
- Use Lambda A100 80GB ($1.99/hr)
- Full fine-tuning (50 hours)
- Multiple experiments
- Total: ~$700

---

## ğŸ¯ Next Steps

1. **This Week:**
   - Start collecting gameplay screenshots
   - Install dataset_builder.py
   - Begin annotation

2. **This Month:**
   - Collect 500+ samples
   - Setup cloud GPU account
   - Run baseline evaluation

3. **Next 2-3 Months:**
   - Complete 1000+ sample dataset
   - Fine-tune with LoRA
   - Deploy custom model

---

## ğŸ“š Resources

**Training Frameworks:**
- Hugging Face Transformers: https://huggingface.co/docs/transformers
- PEFT (LoRA): https://github.com/huggingface/peft
- LLaVA: https://github.com/haotian-liu/LLaVA

**Datasets:**
- Vision-Language: https://huggingface.co/datasets
- Gaming: Create your own!

**Communities:**
- Hugging Face Discord: discord.com/invite/JfAtkvEtRb
- r/MachineLearning: reddit.com/r/MachineLearning

---

**Ready to build your custom AI? Start with Phase 1! ğŸš€**
